{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsippy/CKD-CKDu/blob/master/CCAIM_Tutorial_Innovative_Uses_of_Synthetic_Data_Tutorial_Synthcity_Hands_On.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/vanderschaarlab/synthcity/raw/main/docs/logo.png\" alt=\"Synthcity Logo\" width=\"200\"/>\n",
        "\n",
        "# CCAIM Tutorial: Innovative Uses of Synthetic Data Tutorial\n",
        "\n",
        "## Set up runtime\n",
        "We need to set up our runtime, environment and get the files we need. First, we will select \"Runtime > Change runtime type\"  from the menu. Then we should set the Hardware accelerator to a standard GPU (T4 GPU).\n",
        "Now we are ready to run some code. Let's clone the lab repo. Then cd into it. Then we will install our dependencies. This may require you to restart the runtime session."
      ],
      "metadata": {
        "id": "E38Bda9wWDjF"
      },
      "id": "E38Bda9wWDjF"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://github.com/vanderschaarlab/synthetic-data-lab\n",
        "%cd synthetic-data-lab/\n",
        "!pip install -r requirements.txt;"
      ],
      "metadata": {
        "id": "NNXvfWwqUA0B"
      },
      "id": "NNXvfWwqUA0B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"Is CUDA available?\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
        "    print(\"Current GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"No GPU detected.\")\n"
      ],
      "metadata": {
        "id": "0SjOoQ23xMct"
      },
      "id": "0SjOoQ23xMct",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you do the next step, please now restart you runtime with Runtime > Restart session. This will ensure the dependencies are resolved properly in Colab. You do not need to re-run the first cell once the runtime is restarted.\n",
        "\n",
        "Now cd into the Tutorials directory."
      ],
      "metadata": {
        "id": "MtodzBdb6wZa"
      },
      "id": "MtodzBdb6wZa"
    },
    {
      "cell_type": "code",
      "source": [
        "%cd synthetic-data-lab/\n",
        "%cd Tutorials/\n",
        "!pwd"
      ],
      "metadata": {
        "id": "EcvE76eK6tBK"
      },
      "id": "EcvE76eK6tBK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now set up and ready to start the tutorial."
      ],
      "metadata": {
        "id": "-k8VJi3vXj1r"
      },
      "id": "-k8VJi3vXj1r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthcity: an open-source library to facilitate innovative uses of synthetic data\n",
        "\n",
        "\n",
        "In order to enable wider adoption of synthetic data and facilitate translational research in this promising area, the community needs a software platform that implements a large collection of state-of-the-art generators in a modular, reusable and composable way.\n",
        "\n",
        "Synthcity is an open-source software package for innovative use cases of synthetic data in ML fairness, privacy and augmentation across diverse tabular data modalities, including static data, regular and irregular time series, data with censoring, multi-source data, composite data, and more. Synthcity provides the practitioners with a single access point to cutting edge research and tools in synthetic data. It also offers the community a playground for rapid experimentation and prototyping, a one-stop-shop for SOTA benchmarks, and an opportunity for extending research impact.\n",
        "\n",
        "For researcher:\n",
        " - A playground for rapid experimentation and prototyping\n",
        " - A one-stop-shop for SOTA benchmarks\n",
        " - An opportunity for extending reearch impact and translational research\n",
        "\n",
        "For practitioners:\n",
        " - A way to brining cutting edge research to practical problems\n",
        " - A unified solution to diverse problems\n",
        "\n",
        "\n",
        "Know more about Synthcity by visiting [GitHub](https://github.com/vanderschaarlab/synthcity), reading our [White Paper](https://arxiv.org/abs/2301.07573), or our Paper from [NeurIPS 2023](https://openreview.net/pdf?id=uIppiU2JKP).\n",
        "\n",
        "# Introduction to synthetic data workflow\n",
        "\n",
        "Today we will use Synthcity to illustrate the different use cases of synthetic data in ML and data analytics.\n",
        "\n",
        "![Standard workflow of generating and evaluating synthetic data with synthcity.](https://drive.google.com/uc?export=view&id=1tuRfTk9WQZRD9Be_Szzna8mk4zskAR7_)\n",
        "\n",
        "The synthcity library captures the entire workflow of synthetic data generation and evaluation. The typical workflow contains the following steps, as illustrated above.\n",
        "\n",
        "1. **Loading the dataset using a DataLoader**. The DataLoader class provides a consistent interface for loading and storing different types of input data (e.g. tabular, time series, and survival data). The user can also provide meta-data to inform downstream algorithms (e.g. specifying the sensitive columns for privacy-preserving algorithms).\n",
        "2. **Training the generator using a Plugin**. In synthcity, the users instantiate, train, and apply different data generators via the Plugin class. Each Plugin represents a specific data generation algorithm. The generator can be trained using the fit() method of a Plugin.\n",
        "3. **Generating synthetic data**. After the Plugin is trained, the user can use the generate() method to generate synthetic data. Some plugins also allow for conditional generation.\n",
        "4. **Evaluating synthetic data**. Synthcity provides a large set of metrics for evaluating the fidelity, utility, and privacy of synthetic data. The Metrics class allows users to perform evaluation.\n",
        "\n",
        "In addition, synthcity also has a Benchmark class that wraps around all the four steps, which is helpful for comparing and evaluating different generators.\n",
        "After the synthetic data is evaluated, it can then be used in various downstream tasks."
      ],
      "metadata": {
        "id": "T1ZkiKQOT-bn"
      },
      "id": "T1ZkiKQOT-bn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now set up and ready to start the tutorial."
      ],
      "metadata": {
        "id": "U3cbcAknWKXK"
      },
      "id": "U3cbcAknWKXK"
    },
    {
      "cell_type": "markdown",
      "id": "97e2d93c",
      "metadata": {
        "id": "97e2d93c"
      },
      "source": [
        "# 1. Case Study 1 - Data Modality"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Introduction\n",
        "![Catgorization of data modalities](https://drive.google.com/uc?export=view&id=1nTejenRyLAXv2mwaZdUE63kQc3H2J822)\n",
        "\n",
        "\\\"Tabular data\\\" is a general category that encompasses many different data modalities. In this section, we introduce how to categorize these diverse modalities and how synthcity can be used to handle it.\n",
        "\n",
        "### Single dataset\n",
        "\n",
        "We start by introducing the most fundamental case where there is a single training dataset (e.g. a single DataFrame in Pandas). We characterize the data modalities by two axes: the observation pattern and the feature type.\n",
        "\n",
        "The observation pattern describes whether and how the data are collected over time. There are three most prominent patterns, all supported by synthcity:\n",
        "\n",
        "1. Static. All features are observed in a snapshot. There is no temporal ordering.\n",
        "2. Regular time series.  Observations are made at regular intervals, t = 1, 2, 3... Of note, it is possible that different series may have different number of observations.\n",
        "3. Irregular time series. Observations are made at irregular intervals, t = t1, t2, t3, ... Note that, for different series, the observation times may vary.\n",
        "\n",
        "The feature type describes the domain of individual features. Synthcity supports the following three types. It also supports multivariate cases with a mixture of different feature types.\n",
        "\n",
        "1. Continuous feature\n",
        "2. Categorical feature\n",
        "3. Integer feature\n",
        "4. Censored feature: survival time and censoring indicator\n",
        "\n",
        "The combination of observation patterns and feature types give rise to an array of data modalities. Synthcity supports all combinations.\n",
        "\n",
        "### Composite dataset\n",
        "\n",
        "A composite dataset involves multiple sub datasets. For instance, it may contain datasets collected from different sources or domains (e.g. from different countries). It may also contain both static and time series data. Such composite data are quite often seen in practice. For example, a patient's medical record may contain both static demographic information and longitudinal follow up data.\n",
        "\n",
        "synthcity can handle the generation of different classes of composite datasets. Currently, it supports (1) multiple static datasets, (2) a static and a regular time series dataset, and (3) a static and a irregular time series dataset.\n",
        "\n",
        "### Metadata\n",
        "\n",
        "Very often we have access to metadata that describes the properties of the underlying data. Synthcity can make use of these information to guide the generation and evaluation process. It supports the following types of metadata:\n",
        "\n",
        "1. sensitive features: indicator of sensitive features that should be protected for privacy.\n",
        "2. outcome features: indicator of outcome feature that will be used as the  target in downstream prediction tasks.\n",
        "3. domain: information about the data type and allowed value range.\n"
      ],
      "metadata": {
        "id": "oIGuVkF7WSQ6"
      },
      "id": "oIGuVkF7WSQ6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1.2 The Task\n",
        "In this first exercise, we will get used to loading datasets with the library and generating synthetic data from them, whatever the modality of the real data."
      ],
      "metadata": {
        "id": "XF0LPK6uWVUe"
      },
      "id": "XF0LPK6uWVUe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Imports\n",
        "Lets get the imports out of the way. We import the required standard and 3rd party libraries and relevant Synthcity modules. We can also set the level of logging here, using Synthcity's bespoke logger."
      ],
      "metadata": {
        "id": "eYO3dgXBWX0X"
      },
      "id": "eYO3dgXBWX0X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "696e0157",
      "metadata": {
        "scrolled": false,
        "id": "696e0157"
      },
      "outputs": [],
      "source": [
        "# Standard\n",
        "import sys\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# 3rd party\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# synthcity\n",
        "import synthcity.logger as log\n",
        "from synthcity.plugins import Plugins\n",
        "from synthcity.plugins.core.dataloader import (GenericDataLoader, SurvivalAnalysisDataLoader, TimeSeriesDataLoader, TimeSeriesSurvivalDataLoader)\n",
        "\n",
        "# Configure warnings and logging\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set the level for the logging\n",
        "# log.add(sink=sys.stderr, level=\"INFO\")\n",
        "log.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9ccba34",
      "metadata": {
        "id": "e9ccba34"
      },
      "source": [
        "## 1.4 Loading data of different modalities\n",
        "\n",
        "In this notebook we will load different datasets into synthcity and show that data of many different modalities can be used to generate synthetic data using this module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.1 Static Data\n",
        "Now we will start with the simplest example, static tabular data. For this, we will use the diabetes dataset from sklearn. First, we need to load the dataset."
      ],
      "metadata": {
        "id": "yaQUalI9XBeZ"
      },
      "id": "yaQUalI9XBeZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "660f2c8e",
      "metadata": {
        "id": "660f2c8e"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
        "X[\"target\"] = y\n",
        "display(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7e79722",
      "metadata": {
        "id": "a7e79722"
      },
      "source": [
        "Then we pass it to the `GenericDataLoader` object from `synthcity`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51685173",
      "metadata": {
        "id": "51685173"
      },
      "outputs": [],
      "source": [
        "loader = GenericDataLoader(\n",
        "    X,\n",
        "    target_column=\"target\",\n",
        "    sensitive_columns=[\"sex\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc6fb71d",
      "metadata": {
        "id": "bc6fb71d"
      },
      "source": [
        "We can print out different methods that are compatible with our data by calling `Plugins().list()` with a relevant list passed to the categories parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb7e181c",
      "metadata": {
        "id": "bb7e181c"
      },
      "outputs": [],
      "source": [
        "print(Plugins(categories=[\"generic\"]).list())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42a3735d",
      "metadata": {
        "id": "42a3735d"
      },
      "source": [
        "No need to worry about the code in this next block here, we will go into lots of detail in how to generate synthetic data in the case studies to come. It is here purely to demonstrate that our dataset can be used to generate synthetic data using the synthcity module. We are using the method `marginal_distributions` to generate the synthetic data, which is one of the available debugging methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "227a53e1",
      "metadata": {
        "id": "227a53e1"
      },
      "outputs": [],
      "source": [
        "syn_model = Plugins().get(\"marginal_distributions\")\n",
        "syn_model.fit(loader)\n",
        "syn_model.generate(count=10).dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a66d521f",
      "metadata": {
        "id": "a66d521f"
      },
      "source": [
        "### 1.4.2 Static survival\n",
        "Next lets look at censored data. Censoring is a form of missing data problem in which time to event is not observed for reasons such as termination of study before all recruited subjects have shown the event of interest or the subject has left the study prior to experiencing an event. Censoring is common in survival analysis. For our next example we will load a static survival dataset. Our dataset this time is a veteran lung cancer dataset provided by scikit-survival.\n",
        "\n",
        "First, load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2186c17",
      "metadata": {
        "id": "e2186c17"
      },
      "outputs": [],
      "source": [
        "from sksurv.datasets import load_veterans_lung_cancer # If this causes an error you may need to restart the runtime\n",
        "\n",
        "data_x, data_y = load_veterans_lung_cancer()\n",
        "data_x[\"status\"], data_x[\"survival_in_days\"] = [record[0] for record in data_y], [record[1] for record in data_y]\n",
        "display(data_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fef8d763",
      "metadata": {
        "id": "fef8d763"
      },
      "source": [
        "Pass it to the DataLoader. This time we will use the `SurvivalAnalysisDataLoader`. We need to pass it the data, the name of the column that contains our labels or targets to `target_column` and the the name of the column  containing the time elapsed when the event occurred (the event defined by the target column) to `time_to_event_column`. Calling `info()` on the loader object allows us to see the information about the dataset we have just prepared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db8eea8",
      "metadata": {
        "id": "8db8eea8"
      },
      "outputs": [],
      "source": [
        "loader = SurvivalAnalysisDataLoader(\n",
        "    data_x,\n",
        "    target_column=\"status\",\n",
        "    time_to_event_column=\"survival_in_days\",\n",
        ")\n",
        "display(loader.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20fc3730",
      "metadata": {
        "id": "20fc3730"
      },
      "source": [
        "If we get the `marginal_distributions` plugin again and fit it to the `loader` object, we can then call `generate` to produce the synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36d0ee9d",
      "metadata": {
        "id": "36d0ee9d"
      },
      "outputs": [],
      "source": [
        "syn_model = Plugins().get(\"marginal_distributions\")\n",
        "syn_model.fit(loader)\n",
        "syn_model.generate(count=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "123aec05",
      "metadata": {
        "id": "123aec05"
      },
      "source": [
        "### 1.4.3 Regular Time Series\n",
        "\n",
        "In this next example we will load up a simple regular time series dataset and show that it is compatible with Synthcity. The temporal data must be passed to the loader as a list of DataFrames, where each DataFrame in the list refers to a different record and contains all time points for the record. So, there is a small amount of pre-processing to get our data into the right shape. As it is a regular time series we can simply pass a sequential list for each record.\n",
        "\n",
        "The dataset we will use here is the basic motions dataset provided by SKTime. So, we need to import the library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee6cb4f4",
      "metadata": {
        "id": "ee6cb4f4"
      },
      "outputs": [],
      "source": [
        "from tslearn.datasets import UCR_UEA_datasets\n",
        "\n",
        "# Initialize the dataset loader\n",
        "ucr = UCR_UEA_datasets()\n",
        "\n",
        "# Load the \"BasicMotions\" dataset\n",
        "X_train, y_train, X_test, y_test = ucr.load_dataset(\"BasicMotions\")\n",
        "\n",
        "# Combine train and test splits\n",
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "y = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2f8600",
      "metadata": {
        "id": "da2f8600"
      },
      "source": [
        "Load the data and re-format it into a list of DataFrames, where each DataFrame in the list refers to a different record and contains all time points for the record. We also need the outcomes as a DataFrame and the observation times as a list of time steps for each record. As this is a regular time series our time steps can simply be a sequential list of integers. We will also print the some of the data when we have it in the correct shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26da9e32",
      "metadata": {
        "id": "26da9e32"
      },
      "outputs": [],
      "source": [
        "# Convert the data to a multi-index DataFrame format\n",
        "def convert_to_multiindex_dataframe(X):\n",
        "    \"\"\"Converts numpy array to a pandas DataFrame with multi-index for time series data.\"\"\"\n",
        "\n",
        "\n",
        "    # Create multi-index: first level is instance, second level is time step\n",
        "    index = pd.MultiIndex.from_product([range(n_instances), range(n_timesteps)],\n",
        "                                       names=[\"instance\", \"time_step\"])\n",
        "\n",
        "    # Reshape the data into a long format\n",
        "    data_reshaped = X.reshape(n_instances * n_timesteps, n_features)\n",
        "\n",
        "    # Create the DataFrame with multi-index\n",
        "    df = pd.DataFrame(data_reshaped, index=index, columns=[f\"feature_{i}\" for i in range(n_features)])\n",
        "    return df\n",
        "\n",
        "n_instances, n_timesteps, n_features = X.shape\n",
        "\n",
        "# Convert X to multi-index DataFrame\n",
        "X_multiindex_df = convert_to_multiindex_dataframe(X)\n",
        "\n",
        "# Convert y to a simple DataFrame for consistency\n",
        "y_df = pd.DataFrame(y, columns=[\"label\"], index=pd.Index(range(len(y)), name=\"instance\"))\n",
        "\n",
        "# Now, X_multiindex_df and y_df are in the desired multi-index format\n",
        "print(X_multiindex_df.head())\n",
        "print(y_df.head())\n",
        "\n",
        "# Convert multi-index dataframe into list of dataframes\n",
        "temporal_data = [X_multiindex_df.loc[i] for i in range(len(X))]  # Slice rows by instance\n",
        "y = pd.DataFrame(y, columns=[\"label\"])\n",
        "observation_times = [list(range(X.shape[1])) for _ in range(X.shape[0])]\n",
        "\n",
        "print(\"The first 3 dataframes in the list, `temporal_data`. They refer to the first 3 instances in the dataset. Each instance contains all time steps for all features.\")\n",
        "for i in range(3):\n",
        "    display(temporal_data[i])\n",
        "print(\"\\nThe first 3 label values, `y`.\")\n",
        "display(y[0:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb4f6c46",
      "metadata": {
        "id": "eb4f6c46"
      },
      "source": [
        "Pass the data we just prepared to the DataLoader. Here we will use the `TimeSeriesDataLoader`. Then we will print out the loader info to check everything looks correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9065179e",
      "metadata": {
        "id": "9065179e"
      },
      "outputs": [],
      "source": [
        "loader = TimeSeriesDataLoader(\n",
        "    temporal_data=temporal_data,\n",
        "    observation_times=observation_times,\n",
        "    outcome=y,\n",
        ")\n",
        "display(loader.dataframe())\n",
        "print(loader.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b62d94",
      "metadata": {
        "id": "87b62d94"
      },
      "source": [
        "Now we are ready to produce the synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca58d178",
      "metadata": {
        "id": "ca58d178"
      },
      "outputs": [],
      "source": [
        "syn_model = Plugins().get(\"marginal_distributions\")\n",
        "syn_model.fit(loader)\n",
        "syn_model.generate(count=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46472820",
      "metadata": {
        "id": "46472820"
      },
      "source": [
        "### 1.4.4 Irregular Time Series\n",
        "\n",
        "Now lets load an irregular time series dataset and show that that is also compatible with Synthcity. The dataset we will use here is a google stocks dataset provided by the synthcity module itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98b0f2c1",
      "metadata": {
        "id": "98b0f2c1"
      },
      "outputs": [],
      "source": [
        "from synthcity.utils.datasets.time_series.google_stocks import GoogleStocksDataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dfdddcd",
      "metadata": {
        "id": "9dfdddcd"
      },
      "source": [
        "Try to generate synthetic data for the Google stocks data yourself. Use the previous example as a clue. If you need help with `GoogleStocksDataLoader`, it is defined [here](https://github.com/vanderschaarlab/synthcity/blob/main/src/synthcity/utils/datasets/time_series/google_stocks.py). And the docs for the Synthcity DataLoaders are available [here](https://synthcity.readthedocs.io/en/latest/generated/synthcity.plugins.core.dataloader.html#synthcity.plugins.core.dataloader.TimeSeriesDataLoader). Once you have your answer, compare it to the code hidden in the cell labelled 1.4.4 (i) below.\n",
        "\n",
        "Please use the empty cell immediately below for your answer."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NEv0U9z9aEf2"
      },
      "id": "NEv0U9z9aEf2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "783b02cd",
      "metadata": {
        "id": "783b02cd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title (i) Create Synthetic Data for Irregular Time Series\n",
        "static_data, temporal_data, observation_times, outcome = GoogleStocksDataloader().load()\n",
        "\n",
        "print(f\"static_data | type:{type(static_data)} | shape {static_data.shape}\")\n",
        "print(f\"temporal_data | type:{type(temporal_data)} | type:{type(temporal_data[0])} |shape {temporal_data[0].shape}\")\n",
        "print(f\"observation_times | type:{type(observation_times)} | type:{type(observation_times[0])} | shape {len(observation_times)}\")\n",
        "print(f\"outcome | type:{type(outcome)} | shape {outcome.shape}\")\n",
        "print()\n",
        "\n",
        "loader = TimeSeriesDataLoader(\n",
        "    temporal_data=temporal_data,\n",
        "    observation_times=observation_times,\n",
        "    static_data=static_data,\n",
        "    outcome=outcome,\n",
        ")\n",
        "print(loader.info())\n",
        "display(loader.dataframe())\n",
        "\n",
        "# Exactly as for the regular time series, we can now generate synthetic data,\n",
        "# by selecting our time series compatible plugin, then calling `fit()` and\n",
        "# `generate()`.\n",
        "syn_model = Plugins().get(\"marginal_distributions\")\n",
        "syn_model.fit(loader)\n",
        "syn_model.generate(count=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "229d9fbe",
      "metadata": {
        "id": "229d9fbe"
      },
      "source": [
        "## 1.5 Extension\n",
        "\n",
        "Use the code block below as a space to complete the extension exercises below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.1 Create synthetic datasets\n",
        "\n",
        "Generate synthetic data for another dataset of your choice using the methods described above. You can use any of the other dataset from the sources we have used above: [SKLearn](https://scikit-learn.org/stable/datasets/toy_dataset.html), [SKTime](https://www.sktime.org/en/stable/api_reference/datasets.html), [SKSurv](https://scikit-survival.readthedocs.io/en/stable/api/datasets.html)  or [synthcity](https://github.com/vanderschaarlab/synthcity/tree/main/src/synthcity/utils/datasets) itself. Why not try a composite dataset including both static and temporal features?"
      ],
      "metadata": {
        "id": "AnfPOeUT4wQa"
      },
      "id": "AnfPOeUT4wQa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef2a9ba0",
      "metadata": {
        "id": "ef2a9ba0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.2 What is the best value for n_iter?\n",
        "<details>\n",
        "<summary>Show answer</summary>\n",
        "It depends on your use case. The larger the value the longer it will take to run, however the plugins are equipped with early stopping, so that when a specified metric converges, the GAN stops at that point. So, setting an arbitrarily large value, is often a good option.\n",
        "</details>"
      ],
      "metadata": {
        "id": "XwWodHi0EEp0"
      },
      "id": "XwWodHi0EEp0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Case Study 2 - Fairness\n",
        "If you are running this case study without having also run the previous ones, make sure you have set up the runtime correctly, by running the \"Set up runtime\" and \"Install requirements\" cells at the top of the notebook."
      ],
      "metadata": {
        "id": "mCX2hPceiAet"
      },
      "id": "mCX2hPceiAet"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Introduction\n",
        "One common problem with some machine learning models is an unfair bias in the training data leading to a models that systematically perform worse for some populations. In this case study we will address the issue of fairness by reducing the bias in a generated synthetic dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "bnE0IpHdiDqQ"
      },
      "id": "bnE0IpHdiDqQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 The Task\n",
        "Train a fair prognostic classifier for COVID-19 patients in Brazil."
      ],
      "metadata": {
        "id": "yZQhFZTGiL7N"
      },
      "id": "yZQhFZTGiL7N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Imports\n",
        "Lets import all the modules we need for the second case study. Some we have already imported, but this means you can run work through the case studies in any order, if you want to revisit them after the lab. We import the required standard and 3rd party libraries and relevant Synthcity modules. We can also set the level of logging here, using Synthcity's bespoke logger."
      ],
      "metadata": {
        "id": "YJ8nGfDVnwR_"
      },
      "id": "YJ8nGfDVnwR_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard\n",
        "import sys\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Any, Tuple\n",
        "import itertools\n",
        "\n",
        "# 3rd party\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import networkx as nx\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from graphviz import Digraph\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# synthcity\n",
        "import synthcity\n",
        "import synthcity.logger as log\n",
        "from synthcity.utils import serialization\n",
        "from synthcity.plugins import Plugins\n",
        "from synthcity.metrics import Metrics\n",
        "from synthcity.plugins.core.dataloader import (GenericDataLoader, SurvivalAnalysisDataLoader)\n",
        "from synthcity.plugins.privacy.plugin_decaf import plugin as decaf_plugin\n",
        "from synthcity.plugins.core.constraints import Constraints\n",
        "\n",
        "# # Synthetic-data-lab\n",
        "from utils import fairness_scores\n",
        "from utils import plot_dag\n",
        "\n",
        "# Configure warnings and logging\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set the level for the logging\n",
        "log.remove()\n",
        "# log.add(sink=sys.stderr, level=\"INFO\")\n",
        "\n",
        "\n",
        "# Set up paths to resources\n",
        "FAIR_RES_PATH = Path().cwd() / \"/resources/fairness/\""
      ],
      "metadata": {
        "id": "dYxV0ZobiVHX"
      },
      "id": "dYxV0ZobiVHX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Load the data\n",
        "Next, we can load the data from file and formulate it as a classification problem. To do this we can simply set a time horizon and create an \"is_dead_at_time_horizon\" column."
      ],
      "metadata": {
        "id": "jcSM86S0jWMv"
      },
      "id": "jcSM86S0jWMv"
    },
    {
      "cell_type": "code",
      "source": [
        "time_horizon = 14\n",
        "X = pd.read_csv(f\"../data/Brazil_COVID/covid_normalised_numericalised.csv\")\n",
        "\n",
        "X.loc[(X[\"Days_hospital_to_outcome\"] <= time_horizon) & (X[\"is_dead\"] == 1), f\"is_dead_at_time_horizon={time_horizon}\"] = 1\n",
        "X.loc[(X[\"Days_hospital_to_outcome\"] > time_horizon), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
        "X.loc[(X[\"is_dead\"] == 0), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
        "X[f\"is_dead_at_time_horizon={time_horizon}\"] = X[f\"is_dead_at_time_horizon={time_horizon}\"].astype(int)\n",
        "\n",
        "X.drop(columns=[\"is_dead\", \"Days_hospital_to_outcome\"], inplace=True) # drop survival columns as they are not needed for a classification problem\n",
        "display(X)\n",
        "\n",
        "# Define the mappings of the encoded values in the Ethnicity column to the understandable values\n",
        "ethnicity_mapper = {\n",
        "    0: \"Mixed\",\n",
        "    1: \"White\",\n",
        "    2: \"Black\",\n",
        "    3: \"East Asian\",\n",
        "    4: \"Indigenous\",\n",
        "}"
      ],
      "metadata": {
        "id": "rP1RRjA5jY5Z"
      },
      "id": "rP1RRjA5jY5Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Potential issue\n",
        "\n",
        "The Brazilian population is made up of people of different ethnicities in different proportions. We should check the frequency for each ethnicity to see how evenly distributed our data is across ethnicity. Lets create a plot."
      ],
      "metadata": {
        "id": "duxLn74AjcAx"
      },
      "id": "duxLn74AjcAx"
    },
    {
      "cell_type": "code",
      "source": [
        "ethnicity_frequency_data = pd.DataFrame(\n",
        "    data=X[\"Ethnicity\"].value_counts().rename(ethnicity_mapper),\n",
        ").reset_index().rename(\n",
        "    columns={\"index\": \"Ethnicity\"}  # Rename the index column to \"Ethnicity\"\n",
        ")\n",
        "display(pd.DataFrame(\n",
        "    data=X[\"Ethnicity\"].value_counts().rename(ethnicity_mapper),\n",
        "))\n",
        "sns.barplot(data=ethnicity_frequency_data, x=\"Ethnicity\", y=\"count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H5Lgz8EIjj95"
      },
      "id": "H5Lgz8EIjj95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The population in our dataset is overwhelmingly white and mixed, with little representation of black, East Asian and indigenous people. This poses a problem for us."
      ],
      "metadata": {
        "id": "QNHS333fjquJ"
      },
      "id": "QNHS333fjquJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 The Problem\n",
        "We need a prognostic classifier for the whole population. Having little representation from some parts of the population means that any classifier we train on this data is going to be susceptible to bias. Lets test an `RandomForestClassifier` classifier on the whole dataset then test it on each ethnicity. This will show us the extent of the problem, as we will be able to see any disparity between model performance across the different groups.\n"
      ],
      "metadata": {
        "id": "Kwa_v6vMjtnp"
      },
      "id": "Kwa_v6vMjtnp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.6.1 set up the data\n",
        "Set up the data splits, using train_test_split from sklearn, for a prognostic classifier."
      ],
      "metadata": {
        "id": "uU5AsOJxjwxJ"
      },
      "id": "uU5AsOJxjwxJ"
    },
    {
      "cell_type": "code",
      "source": [
        "y = X[\"is_dead_at_time_horizon=14\"]\n",
        "X_in = X.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_in, y, random_state=4, train_size=0.8)\n",
        "\n",
        "X_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "Aau2otwYj1BC"
      },
      "id": "Aau2otwYj1BC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.2 Load the classifier\n",
        "Load the trained `RandomForestClassifier`, which has been trained on the whole dataset."
      ],
      "metadata": {
        "id": "n-fTaua6j4B1"
      },
      "id": "n-fTaua6j4B1"
    },
    {
      "cell_type": "code",
      "source": [
        "prognostic_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    criterion='gini',\n",
        "    max_depth=None,\n",
        "    random_state=42,\n",
        "    verbose=0,\n",
        "    warm_start=False,\n",
        ")\n",
        "\n",
        "saved_model_path = \"/content/synthetic-data-lab/resources/fairness/fairness_cond_aug_random_forest_real_data.sav\"\n",
        "\n",
        "# # The saved model was trained with the following code\n",
        "prognostic_model.fit(X_train, y_train)\n",
        "with open(saved_model_path, 'wb') as f:\n",
        "    pickle.dump(prognostic_model, f)\n",
        "\n",
        "# Load the model trained on the whole dataset\n",
        "# with open(saved_model_path, \"rb\") as f:\n",
        "#     prognostic_model = pickle.load(f)"
      ],
      "metadata": {
        "id": "066ZpHVHj8SS"
      },
      "id": "066ZpHVHj8SS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.3 Evaluate the classifier\n",
        "Evaluate the overall accuracy of the classifier on the whole dataset. We can see the accuracy on both the train and test sets. This notebook is set up for you to select a preferred performance score of either sklearn's `accuracy_score` or `f1_score`. Feel free to set `performance_score` to either in the line below, or a metric of you choice!"
      ],
      "metadata": {
        "id": "w2WYnptFkDTs"
      },
      "id": "w2WYnptFkDTs"
    },
    {
      "cell_type": "code",
      "source": [
        "performance_score = accuracy_score\n",
        "calculated_performance_score = performance_score(y_train, prognostic_model.predict(X_train))\n",
        "print(f\"Evaluating accuracy on train set: {calculated_performance_score:0.4f}\")\n",
        "\n",
        "# Predicted values for whole dataset\n",
        "y_pred = prognostic_model.predict(X_test)\n",
        "\n",
        "calculated_performance_score = performance_score(y_test, y_pred)\n",
        "print(f\"Evaluating accuracy on test set: {calculated_performance_score:0.4f}\")"
      ],
      "metadata": {
        "id": "rP32Ms_ykGUS"
      },
      "id": "rP32Ms_ykGUS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.4 Confusion Matrices\n",
        "Create the confusion matrix for each of the ethnicities and the whole dataset to compare."
      ],
      "metadata": {
        "id": "3kado0u7kJTA"
      },
      "id": "3kado0u7kJTA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the figure axis\n",
        "f, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
        "\n",
        "# Create the whole dataset confusion matrix and add it to the figure\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=prognostic_model.classes_)\n",
        "disp.plot(ax=axes[0,0])\n",
        "disp.ax_.set_title(f\"Whole test dataset: {calculated_performance_score}\")\n",
        "\n",
        "# Get the indices to loop through\n",
        "ethnicity_idxes = X_in[\"Ethnicity\"].unique()\n",
        "ethnicity_idxes.sort()\n",
        "\n",
        "# for each ethnicity create a confusion matrix\n",
        "for ethnicity_idx in ethnicity_idxes:\n",
        "    # Get the slice of the dataset for each ethnicity\n",
        "    X_test_per_ethnicity = X_test.loc[X_test[\"Ethnicity\"] == ethnicity_idx]\n",
        "    test_records_per_ethnicity_indices = X_test_per_ethnicity.index\n",
        "    y_true = y_test.iloc[test_records_per_ethnicity_indices]\n",
        "\n",
        "    # Generate prediction values for each ethnicity subpopulation\n",
        "    y_pred_per_ethnicity = prognostic_model.predict(X_test_per_ethnicity)\n",
        "\n",
        "    # Calculate the model performance for each ethnicity subpopulation\n",
        "    calculated_performance_score = performance_score(y_true, y_pred_per_ethnicity)\n",
        "\n",
        "    # Generate the confusion matrix and add it to the figure\n",
        "    cm = confusion_matrix(y_true, y_pred_per_ethnicity)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=prognostic_model.classes_)\n",
        "    ax_index = [0, ethnicity_idx + 1] if ethnicity_idx <= 1 else [1, (ethnicity_idx + 1) % 3]\n",
        "    disp.plot(ax=axes[ax_index[0], ax_index[1]])\n",
        "    disp.ax_.set_title(f\"Ethnicity: {ethnicity_mapper[ethnicity_idx]} | Performance: {calculated_performance_score:0.4f}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bz9ewiMMkMVC"
      },
      "id": "Bz9ewiMMkMVC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the performance of the model on the black population is significantly worse than the overall performance. Interestingly, however the model performs better on the East Asian subpopulation. This is likely to be due to random chance, i.e. it happens that the East Asian patients in this sample had features that are good predictors of the outcome, but this would not necessarily be true for a bigger sample from the same population. The Indigenous population is so poorly represented in the dataset, with only 3 records, that it is difficult to even accurately assess performance. However, the indication we have from these three records suggests performance may be poor.\n",
        "\n",
        "This confirms by using a naive method like the one above, we would create a model that systematically performs worse for people of one ethnicity compared to another. This unfairness must be addressed."
      ],
      "metadata": {
        "id": "r7t-Mf-8kP_e"
      },
      "id": "r7t-Mf-8kP_e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 The solution - Augment the dataset to improve the fairness"
      ],
      "metadata": {
        "id": "z6cCDVQjkSAb"
      },
      "id": "z6cCDVQjkSAb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7.1 Load the data with the synthcity module\n",
        "First we load the data with the GenericDataLoader. For this we need to pass the names of our `target_column` to the data loader. Then we can see the data by calling loader.dataframe() and we could also get the information about the data loader object with loader.info()."
      ],
      "metadata": {
        "id": "Ulgw3eZrkZTx"
      },
      "id": "Ulgw3eZrkZTx"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[\"is_dead_at_time_horizon=14\"] = y_train\n",
        "loader = GenericDataLoader(\n",
        "    X_train,\n",
        "    target_column=f\"is_dead_at_time_horizon={time_horizon}\",\n",
        "    sensitive_features=[\"Ethnicity\"],\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "display(loader.dataframe())"
      ],
      "metadata": {
        "id": "JSYoDbF-kcIU"
      },
      "id": "JSYoDbF-kcIU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7.2 Load/Create the synthetic data model\n",
        "We are now going to generate synthetic data with a condition such that the new dataset is more balanced with regard to ethnicity. We will first define some values which we will use below."
      ],
      "metadata": {
        "id": "7ws7rCjPkgE4"
      },
      "id": "7ws7rCjPkgE4"
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"ctgan\"\n",
        "prefix = \"fairness.conditional_augmentation\"\n",
        "random_state = 42"
      ],
      "metadata": {
        "id": "5bTbyEhvkjYi"
      },
      "id": "5bTbyEhvkjYi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now create and fit a synthetic data model. The loading bar may suggest this will take a long time, but early stopping should limit it to approximately 7 minutes."
      ],
      "metadata": {
        "id": "yQDup4g6kmt0"
      },
      "id": "yQDup4g6kmt0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define saved model name\n",
        "save_file = Path(\"saved_models\") / f\"{prefix}_{model}_numericalised_rnd={random_state}.bkp\"\n",
        "\n",
        "syn_model = Plugins().get(model, n_iter=100, random_state=random_state)\n",
        "syn_model.fit(loader, cond=loader[\"Ethnicity\"])\n",
        "serialization.save_to_file(save_file, syn_model)"
      ],
      "metadata": {
        "id": "iEcVX0dQkoI9"
      },
      "id": "iEcVX0dQkoI9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7.3 Generate fairer data\n",
        "Use the synthetic data model to generate data using the `cond` argument to try and make the data evenly distributed across ethnicity. We will then augment the original, real dataset with the synthetic records from the under-represented ethnicities."
      ],
      "metadata": {
        "id": "OPsCZ81ylVtY"
      },
      "id": "OPsCZ81ylVtY"
    },
    {
      "cell_type": "code",
      "source": [
        "count = 10000\n",
        "cond = [(i % 3) + 2 for i in range(count)] # set cond to an equal proportion of each  minority index\n",
        "syn_data = syn_model.generate(count=count, cond=cond, random_state=random_state).dataframe()\n",
        "augmented_data = pd.concat([\n",
        "    X_train,\n",
        "    syn_data.loc[syn_data[\"Ethnicity\"] >= 2],\n",
        "])\n",
        "\n",
        "display(augmented_data)\n",
        "\n",
        "print(\"Here is the ethnicity breakdown for the real dataset:\")\n",
        "print(loader[\"Ethnicity\"].value_counts().rename(ethnicity_mapper))\n",
        "print(\"\\nHere is the ethnicity breakdown for the synthetic dataset:\")\n",
        "print(syn_data[\"Ethnicity\"].value_counts().rename(ethnicity_mapper))\n",
        "print(\"\\nHere is the ethnicity breakdown for the augmented dataset:\")\n",
        "print(augmented_data[\"Ethnicity\"].value_counts().rename(ethnicity_mapper))"
      ],
      "metadata": {
        "id": "bLiI_BtwlZHO"
      },
      "id": "bLiI_BtwlZHO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the ethnicity breakdown again now to check we have augmented the under-represented groups properly. This is important as the conditional only optimizes the GAN here it does not guarantee that generated samples perfectly meet that condition. If you require rules to be strictly adhered to, use `Constraints` instead."
      ],
      "metadata": {
        "id": "FMGH_p45ldHm"
      },
      "id": "FMGH_p45ldHm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7.4 Re-evaluate the classifier on the augmented dataset\n",
        "Lets train our classifier again trained on the augmented dataset."
      ],
      "metadata": {
        "id": "DbGhBKovlfEF"
      },
      "id": "DbGhBKovlfEF"
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_y = augmented_data[\"is_dead_at_time_horizon=14\"]\n",
        "augmented_X = augmented_data.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
        "augmented_y.reset_index(drop=True, inplace=True)\n",
        "augmented_X.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "BA1aLbLxliG5"
      },
      "id": "BA1aLbLxliG5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We need a model trained on the new data. We can load this from file, as before."
      ],
      "metadata": {
        "id": "i0sNX75hljxz"
      },
      "id": "i0sNX75hljxz"
    },
    {
      "cell_type": "code",
      "source": [
        "# saved_model_path = \"/content/synthetic-data-lab/resources/fairness/fairness_cond_aug_random_forest_augmented_data.sav\"\n",
        "\n",
        "# The saved model was trained with the following code\n",
        "prognostic_model.fit(augmented_X, augmented_y)\n",
        "with open(saved_model_path, 'wb') as f:\n",
        "    pickle.dump(prognostic_model, f)\n",
        "\n",
        "\n",
        "# # Load the model trained on the whole dataset\n",
        "# with open(saved_model_path, \"rb\") as f:\n",
        "#     prognostic_model = pickle.load(f)"
      ],
      "metadata": {
        "id": "ogJJXKLjllEV"
      },
      "id": "ogJJXKLjllEV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the performance of the model on the real dataset according to the \"train-on-synthetic, test-on-real rule\"."
      ],
      "metadata": {
        "id": "4IilrnjvlpUH"
      },
      "id": "4IilrnjvlpUH"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = prognostic_model.predict(X_test)\n",
        "calculated_performance_score = performance_score(y_test, y_pred)\n",
        "print(f\"evaluating test set: {calculated_performance_score:0.4f}\")"
      ],
      "metadata": {
        "id": "1jg2MipTlq4X"
      },
      "id": "1jg2MipTlq4X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7.5 New confusion matrices\n",
        "Create the confusion matrix for the synthetic dataset."
      ],
      "metadata": {
        "id": "mUM3ePmNluTL"
      },
      "id": "mUM3ePmNluTL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the figure axis\n",
        "f, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
        "\n",
        "# Create the whole dataset confusion matrix and add it to the figure\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=prognostic_model.classes_)\n",
        "disp.plot(ax=axes[0,0])\n",
        "# disp.plot(ax=axes[0])\n",
        "disp.ax_.set_title(f\"Whole Test Dataset | Performance: {calculated_performance_score:0.4f}\")\n",
        "\n",
        "# Get the indices to loop through\n",
        "ethnicity_idxes = augmented_X[\"Ethnicity\"].unique()\n",
        "ethnicity_idxes.sort()\n",
        "\n",
        "# for each ethnicity create a confusion matrix\n",
        "for ethnicity_idx in ethnicity_idxes:\n",
        "    # Get the slice of the dataset for each ethnicity\n",
        "    X_test_per_ethnicity = X_test.loc[X_test[\"Ethnicity\"] == ethnicity_idx]\n",
        "    test_records_per_ethnicity_indicies = X_test_per_ethnicity.index\n",
        "    y_true_per_ethnicity = y_test.iloc[test_records_per_ethnicity_indicies]\n",
        "\n",
        "    # Generate prediction values for each ethnicity subpopulation\n",
        "    y_pred_per_ethnicity = prognostic_model.predict(X_test_per_ethnicity)\n",
        "\n",
        "    # Calculate the model performance for each ethnicity subpopulation\n",
        "    calculated_performance_score = performance_score(y_true_per_ethnicity, y_pred_per_ethnicity)\n",
        "\n",
        "    # Generate the confusion matrix and add it to the figure\n",
        "    cm = confusion_matrix(y_true_per_ethnicity, y_pred_per_ethnicity)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=prognostic_model.classes_)\n",
        "    # disp.plot(ax=axes[ethnicity_idx + 1])\n",
        "    try:\n",
        "      ax_index = [0, ethnicity_idx + 1] if ethnicity_idx <= 1 else [1, (ethnicity_idx + 1) % 3]\n",
        "      disp.plot(ax=axes[ax_index[0], ax_index[1]])\n",
        "      disp.ax_.set_title(f\"Ethnicity: {ethnicity_mapper[ethnicity_idx]} | Performance: {calculated_performance_score:0.4f}\")\n",
        "    except ValueError as e:\n",
        "      print(f\"Not all quadrants contain values for {ethnicity_mapper[ethnicity_idx]}\\n\\n\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yCA2JHM5lzyJ"
      },
      "id": "yCA2JHM5lzyJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can hopefully see the new model trained on the synthetic data performs more similarly across the different populations.\n",
        "\n",
        "Why not play with the different synthetic data generation methods and their parameters to see if you can achieve the same improvement in fairness, but with a higher performance? If you need help identifying the right methods then, remember you can list the available plugins with `Plugins().list()` and to learn what they do refer to the [docs](https://synthcity.readthedocs.io/en/latest/generators.html)."
      ],
      "metadata": {
        "id": "i8dfOqtrl3VO"
      },
      "id": "i8dfOqtrl3VO"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UeRXmkZsmXNH"
      },
      "id": "UeRXmkZsmXNH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Extension - Using inference-time debiassing with DECAF\n",
        "\n",
        "This section (2.8) involves fitting the decaf model, which can take more than 30 minutes, so it is included here as an extension exercise to try after the session, if you don't have time.\n",
        "\n",
        "DECAF is an inference-time de-biasing method, where the data-generating process is embedded explicitly as a structural causal model in the input layers of the generator, allowing each variable to be reconstructed conditioned on its causal parents. We will use this method to create a different solution to the issue of fairness in this Brazilian COVID-19 dataset.\n",
        "\n",
        "In this section, we consider the fairness issue that may arise when one operationalizes a clinical prognositic tool as a triaging system (to decide who is first to admit into the ICU). In this context, the notion of **[demographic parity](https://en.wikipedia.org/wiki/Fairness_(machine_learning)#Definitions_based_on_predicted_outcome)** is likely to be of interest."
      ],
      "metadata": {
        "id": "MLEyA254l679"
      },
      "id": "MLEyA254l679"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8.1 Load the data\n",
        "Lets load the data from file again to make sure we are working with the correct data and nothing has changed. As before we will construct it as a classification problem. But for this excercise we will set the problem up as a classic 2-class fairness task with one class representing the majority ethnic groups (White and Mixed) and one the Minority (Black, East-Asian, and Indigenous)."
      ],
      "metadata": {
        "id": "95IVxym_l9fy"
      },
      "id": "95IVxym_l9fy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from file\n",
        "X = pd.read_csv(f\"../data/Brazil_COVID/covid_normalised_numericalised.csv\")\n",
        "\n",
        "# Set it up as classification task\n",
        "time_horizon = 14\n",
        "X.loc[(X[\"Days_hospital_to_outcome\"] <= time_horizon) & (X[\"is_dead\"] == 1), f\"is_dead_at_time_horizon={time_horizon}\"] = 1\n",
        "X.loc[(X[\"Days_hospital_to_outcome\"] > time_horizon), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
        "X.loc[(X[\"is_dead\"] == 0), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
        "X[f\"is_dead_at_time_horizon={time_horizon}\"] = X[f\"is_dead_at_time_horizon={time_horizon}\"].astype(int)\n",
        "\n",
        "X.drop(columns=[\"is_dead\", \"Days_hospital_to_outcome\"], inplace=True) # drop survival columns as they are not needed for a classification problem\n",
        "\n",
        "# Set up ethnicity as two classes, minority and majority\n",
        "X.loc[(X[\"Ethnicity\"] == 0) | (X[\"Ethnicity\"] == 1), \"Ethnicity\"] = 0\n",
        "X.loc[(X[\"Ethnicity\"] == 2) | (X[\"Ethnicity\"] == 3) | (X[\"Ethnicity\"] == 4), \"Ethnicity\"] = 1"
      ],
      "metadata": {
        "id": "BE1Mu5sNmC-0"
      },
      "id": "BE1Mu5sNmC-0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass the data into the synthcity dataloader. We will just use the `GenericDataloader` here."
      ],
      "metadata": {
        "id": "ov13ymQVmFA7"
      },
      "id": "ov13ymQVmFA7"
    },
    {
      "cell_type": "code",
      "source": [
        "loader = GenericDataLoader(\n",
        "    X,\n",
        "    target_column=\"is_dead_at_time_horizon=14\",\n",
        "    sensitive_features=[\"Ethnicity\"],\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "display(loader.dataframe())"
      ],
      "metadata": {
        "id": "kQeqoJpDmG7u"
      },
      "id": "kQeqoJpDmG7u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8.2 Load/Create the synthetic datamodel using DECAF\n",
        "First, we define some useful variables."
      ],
      "metadata": {
        "id": "hGJ94usnmL2B"
      },
      "id": "hGJ94usnmL2B"
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"fairness.causal_generation\"\n",
        "model = \"decaf\"\n",
        "n_iter = 101\n",
        "count = 6882 # set the count equal to the number of rows in the original dataset\n",
        "random_state=6"
      ],
      "metadata": {
        "id": "4_kmwvgqmQKY"
      },
      "id": "4_kmwvgqmQKY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then load the synthetic model from file. If you want to try something a little different you can also fit your own model."
      ],
      "metadata": {
        "id": "K47rgZVlmTkn"
      },
      "id": "K47rgZVlmTkn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define saved model name\n",
        "save_file = Path(\"saved_models\") / f\"{prefix}_{model}_n_iter={n_iter}_rnd={random_state}.bkp\"\n",
        "\n",
        "syn_model = decaf_plugin(struct_learning_enabled=True, n_iter=n_iter, random_state=random_state) # Pass struct_learning_enabled=True in order for the syn_model to learn the Dag\n",
        "dag_before = syn_model.get_dag(loader.dataframe())\n",
        "syn_model.fit(loader, dag=dag_before)\n",
        "serialization.save_to_file(save_file, syn_model)"
      ],
      "metadata": {
        "id": "sZKm5LhzmVAh"
      },
      "id": "sZKm5LhzmVAh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8.3 Generate the data\n",
        "We can simply generate the de-biased dataset, by passing the biased edges we wish to remove from the data to `generate`."
      ],
      "metadata": {
        "id": "KiCQOCywmYhB"
      },
      "id": "KiCQOCywmYhB"
    },
    {
      "cell_type": "code",
      "source": [
        "bias={\"Ethnicity\": [\"is_dead_at_time_horizon=14\"]}\n",
        "decaf_syn_data = syn_model.generate(count, biased_edges=bias, random_state=14)\n",
        "display(decaf_syn_data.dataframe())"
      ],
      "metadata": {
        "id": "bk2yU5oumlK9"
      },
      "id": "bk2yU5oumlK9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8.4 DECAF fairness tests\n",
        "\n",
        "We will now check the dag for the synthetic data to see if the biased edgr has been removed. You may also wish to test demographic parity or fairness through unawareness metrics. The definitions for which can be seen in section 4.1 of the [DECAF paper](https://arxiv.org/abs/2110.12884)."
      ],
      "metadata": {
        "id": "di1dofQVmnD8"
      },
      "id": "di1dofQVmnD8"
    },
    {
      "cell_type": "code",
      "source": [
        "print(syn_model.get_dag(decaf_syn_data.dataframe()))"
      ],
      "metadata": {
        "id": "WbSsOpgemqhc"
      },
      "id": "WbSsOpgemqhc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.9 Extension to the extension\n",
        "Use the code block below as a space to complete the extension exercises."
      ],
      "metadata": {
        "id": "kQihJytMmsnc"
      },
      "id": "kQihJytMmsnc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.9.1 Plot the DECAF synthetic data to show fairness\n",
        "\n",
        "What is a simple plot we could make to show that the generated data is fair?"
      ],
      "metadata": {
        "id": "tF2kTAjPmu5F"
      },
      "id": "tF2kTAjPmu5F"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9M5NRghYmvpv"
      },
      "id": "9M5NRghYmvpv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (i) Plot to show fairness in data generated by DECAF\n",
        "\n",
        "# 2 class ethnicity mapper\n",
        "ethnicity_mapper_2_class = {\n",
        "    0: \"Majority\",\n",
        "    1: \"Minority\"\n",
        "}\n",
        "\n",
        "# Define the model\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=1,\n",
        "    gamma=1,\n",
        "    objective=\"binary:logistic\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Load the model trained on the whole dataset\n",
        "saved_model_path = \"../resources/fairness/fairness_causal_gen_decaf_synthetic_data.json\"\n",
        "xgb_model.load_model(saved_model_path)\n",
        "synth_data_to_predict = decaf_syn_data.dataframe().drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
        "targets_synth_data_to_predict = decaf_syn_data[\"is_dead_at_time_horizon=14\"]\n",
        "\n",
        "# # The saved model was trained with the following code\n",
        "# xgb_model.fit(synth_data_to_predict, targets_synth_data_to_predict)\n",
        "# xgb_model.save_model(saved_model_path)\n",
        "\n",
        "# Get the indices to loop through\n",
        "ethnicity_idxes = decaf_syn_data[\"Ethnicity\"].unique()\n",
        "ethnicity_idxes.sort()\n",
        "\n",
        "predictions = {}\n",
        "for ethnicity_idx in ethnicity_idxes:\n",
        "    synth_data_to_predict_per_ethnicity = synth_data_to_predict.loc[synth_data_to_predict[\"Ethnicity\"] == ethnicity_idx]\n",
        "    # display(synth_data_to_predict)\n",
        "\n",
        "    synthetic_predictions = xgb_model.predict(synth_data_to_predict)\n",
        "\n",
        "    unique, counts = np.unique(synthetic_predictions, return_counts=True)\n",
        "    prediction_counts = {unique[0]: counts[0], unique[1]: counts[1]}\n",
        "    predictions[ethnicity_mapper_2_class[ethnicity_idx]] = prediction_counts\n",
        "\n",
        "prediction_frequency_data = pd.DataFrame(data={\n",
        "    \"Ethnicity\": predictions.keys(),\n",
        "    \"0\": [p_c[0] for p, p_c in predictions.items()],\n",
        "    \"1\": [p_c[1] for p, p_c in predictions.items()],\n",
        "})\n",
        "\n",
        "prediction_frequency_data_m = pd.melt(prediction_frequency_data, id_vars=\"Ethnicity\")\n",
        "prediction_frequency_data_m = prediction_frequency_data_m.rename(\n",
        "    columns={\"variable\": \"is_dead_at_time_horizon\", \"value\": \"Prediction count\"}\n",
        ")\n",
        "\n",
        "\n",
        "sns.catplot(\n",
        "    data=prediction_frequency_data_m,\n",
        "    x=\"Ethnicity\",\n",
        "    y=\"Prediction count\",\n",
        "    hue=\"is_dead_at_time_horizon\",\n",
        "    kind=\"bar\"\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FJPOaaIym3h6",
        "cellView": "form"
      },
      "id": "FJPOaaIym3h6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Case Study 3 - Privacy\n",
        "If you are running this case study without having also run the previous ones, make sure you have set up the runtime correctly, by running the \"Set up runtime\" and \"Install requirements\" cells at the top of the notebook."
      ],
      "metadata": {
        "id": "05sK9fusnBp8"
      },
      "id": "05sK9fusnBp8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Introduction\n",
        "Machine learning (ML) is empowering more and more communities by using their historical datasets. Unfortunately, some sectors and use cases have been precluded from the benefits of ML, due to the requirement of their data to remain private. In this case study we will look at methods that aim to solve this problem by creating synthetic datasets that are not bound by the constraints of privacy."
      ],
      "metadata": {
        "id": "C3d-xeaMnWS6"
      },
      "id": "C3d-xeaMnWS6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 The Task\n",
        "Make a private version of the Brazil COVID-19 dataset, that could safely be used by anyone to create a COVID-19 survival analysis model, without the risk of (re-)identification of individuals."
      ],
      "metadata": {
        "id": "j9Tae3-lnZHL"
      },
      "id": "j9Tae3-lnZHL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Imports\n",
        "Lets import all the modules we need for the second case study. Some we have already imported, but this means you can run work through the case studies in any order, if you want to revisit them after the lab."
      ],
      "metadata": {
        "id": "AQm1EoTWnY5o"
      },
      "id": "AQm1EoTWnY5o"
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard\n",
        "import sys\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# 3rd party\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# synthcity\n",
        "import synthcity.logger as log\n",
        "from synthcity.utils import serialization\n",
        "from synthcity.plugins import Plugins\n",
        "from synthcity.plugins.core.dataloader import (GenericDataLoader, SurvivalAnalysisDataLoader)\n",
        "from synthcity.metrics import Metrics\n",
        "\n",
        "# Configure warnings and logging\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set the level for the logging\n",
        "# log.add(sink=sys.stderr, level=\"DEBUG\")\n",
        "log.remove()"
      ],
      "metadata": {
        "id": "YFS5J797nYdu"
      },
      "id": "YFS5J797nYdu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Load the data\n",
        "\n",
        "Load the data from file into a SurvivalAnalysisDataLoader object. For this we need to pass the names of our `target_column` and our `time_to_event_column` to the data loader. Then we can see the data by calling loader.dataframe() and get the information about the data loader object with loader.info()."
      ],
      "metadata": {
        "id": "EQsrWmljpZZH"
      },
      "id": "EQsrWmljpZZH"
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.read_csv(f\"../data/Brazil_COVID/covid_normalised_numericalised.csv\")\n",
        "X = X[X.Days_hospital_to_outcome != 0]\n",
        "loader = SurvivalAnalysisDataLoader(\n",
        "    X,\n",
        "    target_column=\"is_dead\",\n",
        "    time_to_event_column=\"Days_hospital_to_outcome\",\n",
        "    sensitive_features=[\"Age\", \"Sex\", \"Ethnicity\", \"Region\"],\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(loader.info())\n",
        "display(loader.dataframe())"
      ],
      "metadata": {
        "id": "5NEZ-ntBpcT1"
      },
      "id": "5NEZ-ntBpcT1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Load/Create synthetic datasets\n",
        "\n",
        "We can list the available synthetic generators by calling list() on the Plugins object."
      ],
      "metadata": {
        "id": "cYupXSHbpvM-"
      },
      "id": "cYupXSHbpvM-"
    },
    {
      "cell_type": "code",
      "source": [
        "print(Plugins().list())"
      ],
      "metadata": {
        "id": "meDWWNcVp0Nb"
      },
      "id": "meDWWNcVp0Nb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above list we are going to select the synthetic generation models for privacy: \"adsgan\", and \"pategan\". Then we will create and fit the synthetic model before using it to generate a synthetic dataset."
      ],
      "metadata": {
        "id": "nM6ZPF54p2C2"
      },
      "id": "nM6ZPF54p2C2"
    },
    {
      "cell_type": "code",
      "source": [
        "outdir = Path(\"saved_models\")\n",
        "prefix = \"privacy\"\n",
        "n_iter = 100\n",
        "random_state=1\n",
        "models={\n",
        "    \"adsgan\":None,\n",
        "    \"pategan\":None,\n",
        "}"
      ],
      "metadata": {
        "id": "ma206eVcp4Qq"
      },
      "id": "ma206eVcp4Qq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Evaluate the generated synthetic dataset in terms of privacy\n",
        "We can select some metrics to choose. The full list of available metrics can be seen by calling Metrics().list(). We are going to use the metrics associated with detection of the synthetic data and data privacy. Then we will print them to a DataFrame to look at the results.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Mt9_dQuhGFGl1XhEL3XADqHr-E0V6NE2\" alt=\"Evaluating Different Aspects of Synthetic Data and Their Metrics\">\n"
      ],
      "metadata": {
        "id": "USUPiGKKqGJf"
      },
      "id": "USUPiGKKqGJf"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = {}\n",
        "for model in models:\n",
        "    syn_model = Plugins().get(model, n_iter=n_iter, random_state=random_state)\n",
        "    syn_model.fit(loader)\n",
        "    models[model] = syn_model\n",
        "    save_file = outdir / f\"{prefix}.{model}_n_iter={n_iter}_rnd={random_state}.bkp\"\n",
        "    serialization.save_to_file(save_file, syn_model)\n",
        "    selected_metrics = {\n",
        "        \"detection\": [\"detection_xgb\", \"detection_mlp\", \"detection_gmm\"],\n",
        "        \"privacy\": [\"k-anonymization\", \"k-map\", \"distinct l-diversity\", \"identifiability_score\"],\n",
        "        'performance': ['linear_model', 'mlp', 'xgb'],\n",
        "    }\n",
        "    my_metrics = Metrics()\n",
        "    selected_metrics_in_my_metrics = {k: my_metrics.list()[k] for k in my_metrics.list().keys() & selected_metrics.keys()}\n",
        "    X_syn = syn_model.generate(count=6882, random_state=1)\n",
        "    evaluation = my_metrics.evaluate(\n",
        "        loader,\n",
        "        X_syn,\n",
        "        task_type=\"survival_analysis\",\n",
        "        metrics=selected_metrics_in_my_metrics,\n",
        "        workspace=\"workspace\",\n",
        "    )\n",
        "    # Select the metrics that we need\n",
        "    display_metrics = [\n",
        "      \"performance.xgb.syn_ood.c_index\",\n",
        "      \"performance.linear_model.syn_ood.c_index\",\n",
        "      \"performance.mlp.syn_ood.c_index\",\n",
        "      \"detection.detection_xgb.mean\",\n",
        "      \"detection.detection_mlp.mean\",\n",
        "      \"detection.detection_gmm.mean\",\n",
        "      \"detection.detection_linear.mean\",\n",
        "      \"privacy.k-anonymization.syn\",\n",
        "      \"privacy.k-map.score\",\n",
        "      \"privacy.distinct l-diversity.syn\",\n",
        "      \"privacy.identifiability_score.score\",\n",
        "    ]\n",
        "\n",
        "    evaluation = evaluation.loc[display_metrics]\n",
        "    display(evaluation)\n",
        "    eval_results[model] = evaluation"
      ],
      "metadata": {
        "id": "WzRorESxqJpg"
      },
      "id": "WzRorESxqJpg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6.1 Display the evalution results\n",
        "The above table contains all the information we need to evaluate the methods, but lets convert it to a format where it is easier to compare the methods."
      ],
      "metadata": {
        "id": "X9al72b1qSo2"
      },
      "id": "X9al72b1qSo2"
    },
    {
      "cell_type": "code",
      "source": [
        "means = []\n",
        "for plugin in eval_results:\n",
        "    data = eval_results[plugin][\"mean\"]\n",
        "    directions = eval_results[plugin][\"direction\"].to_dict()\n",
        "    means.append(data)\n",
        "\n",
        "out = pd.concat(means, axis=1)\n",
        "out = out.set_axis(eval_results.keys(), axis=1)\n",
        "\n",
        "bad_highlight = \"background-color: lightcoral;\"\n",
        "ok_highlight = \"background-color: green;\"\n",
        "default = \"\"\n",
        "\n",
        "\n",
        "def highlights(row):\n",
        "    metric = row.name\n",
        "    if directions[metric] == \"minimize\":\n",
        "        best_val = np.min(row.values)\n",
        "        worst_val = np.max(row)\n",
        "    else:\n",
        "        best_val = np.max(row.values)\n",
        "        worst_val = np.min(row)\n",
        "\n",
        "    styles = []\n",
        "    for val in row.values:\n",
        "        if val == best_val:\n",
        "            styles.append(ok_highlight)\n",
        "        elif val == worst_val:\n",
        "            styles.append(bad_highlight)\n",
        "        else:\n",
        "            styles.append(default)\n",
        "\n",
        "    return styles\n",
        "\n",
        "\n",
        "out.style.apply(highlights, axis=1)"
      ],
      "metadata": {
        "id": "Jevbfcx7qWZs"
      },
      "id": "Jevbfcx7qWZs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7 Results of evaluation\n",
        "We are using three types of metric here: performance, detection and privacy. Performance metrics explain the utility of a synthetic dataset. Detection metrics measure the ability to identify the real data compared to the synthetic data. The privacy metrics measure how easy it would be to re-identify a patient given the quasi-identifying fields in the dataset.\n",
        "Generally, ADSGAN performs best in synthetic data detection and performance tasks, then PATEGAN.\n",
        "\n",
        "k-anonymization - risk of re-identification is approximately 1/k according to [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2528029/). Therefore the risk of re-identification is lowest for ADSGAN then PATEGAN.\n",
        "\n",
        "k-map - is a metric where every combination of values for the quasi-identifiers appears at least k times in the synthetic dataset. ADSGAN performs worse than PATEGAN.\n",
        "\n",
        "l-diversity - is a similar metric to k-anonymization, but ir is also concerned with the diversity of the generalized block. We see broardly the same pattern as for k-anonymization.\n",
        "\n",
        "identifiability_score - Risk of re-identification as defined in [this paper](https://ieeexplore.ieee.org/document/9034117).\n"
      ],
      "metadata": {
        "id": "PEKZNE3oqZO7"
      },
      "id": "PEKZNE3oqZO7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.8 Synthetic Data Quality\n",
        "To get a good sense of the quality of the synthetic datasets and validate our previous conclusion. Lets plot the correlation/strength-of-association of features in data-set with both categorical and continuous features using:\n",
        "- Pearson's R for continuous-continuous cases\n",
        "- Correlation Ratio for categorical-continuous cases\n",
        "- Cramer's V or Theil's U for categorical-categorical cases\n",
        "\n",
        "In each of the following plots we are looking for the synthetic data to be as similar to the real data as possible. That is minimal values for Jensen-Shannon distance and pairwise correlation distance, and T-SNEs with similar looking distribution in the representation space."
      ],
      "metadata": {
        "id": "A7WFBZMuqdK9"
      },
      "id": "A7WFBZMuqdK9"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for model in models:\n",
        "    models[model].plot(plt, loader, plots=[\"associations\",\"marginal\", \"tsne\"])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-xff6dbcqj9C"
      },
      "id": "-xff6dbcqj9C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.9. Extension\n",
        "Use the code block below as a space to complete the extension exercises below."
      ],
      "metadata": {
        "id": "UwHDuNcaqkyP"
      },
      "id": "UwHDuNcaqkyP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.9.1 Training models on both sets of data\n",
        "\n",
        "1) Use the metrics to get a the performance of a model trained on the real dataset to put our performance scores in context.\n",
        "\n",
        "2) Train your own downstream model on both the original dataset and each of the private datasets we have generated to see if you reach the same conclusion. Which privacy method provides the best performance and what are the trade-offs?"
      ],
      "metadata": {
        "id": "pihDQsdeqsGu"
      },
      "id": "pihDQsdeqsGu"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J7yswDiaquI5"
      },
      "id": "J7yswDiaquI5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Case Study 4 - Augmentation\n",
        "If you are running this case study without having also run the previous ones, make sure you have set up the runtime correctly, by running the \"Set up runtime\" and \"Install requirements\" cells at the top of the notebook."
      ],
      "metadata": {
        "id": "4fu5aheXqwv6"
      },
      "id": "4fu5aheXqwv6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Introduction\n",
        "One of the most common problems for machine Learning practitioners is only having a small dataset for the specific problem they are working on. This traditionally can often lead to dead ends or hold-ups in projects while more data is collected. However, if you have different dataset that shares common features then Synthcity may have the ability to solve the issue for you with \"Augmentation by domain adaption\"."
      ],
      "metadata": {
        "id": "pZw5cA-ArFy-"
      },
      "id": "pZw5cA-ArFy-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 The Task\n",
        "Augment a small dataset using the concept of domain adaptation (or transfer learning). For this we will be using a RadialGAN as discussed in [this paper](https://arxiv.org/pdf/1802.06403.pdf)."
      ],
      "metadata": {
        "id": "pUP-y7BkrJ_v"
      },
      "id": "pUP-y7BkrJ_v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Imports\n",
        "Lets import all the modules we need for the second case study. Some we have already imported, but this means you can run work through the case studies in any order, if you want to revisit them after the lab."
      ],
      "metadata": {
        "id": "BP4dfA5YrQXn"
      },
      "id": "BP4dfA5YrQXn"
    },
    {
      "cell_type": "code",
      "source": [
        "# stdlib\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# 3rd Party\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import auc, accuracy_score\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# synthcity absolute\n",
        "import synthcity.logger as log\n",
        "from synthcity.plugins import Plugins\n",
        "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
        "from synthcity.utils import serialization\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Set the level for the logging\n",
        "# log.add(sink=sys.stderr, level=\"DEBUG\")\n",
        "log.remove()\n",
        "\n",
        "# Set up paths to resources\n",
        "AUG_RES_PATH = Path(\"../resources/augmentation/\")"
      ],
      "metadata": {
        "id": "mUBRUihGrVlR"
      },
      "id": "mUBRUihGrVlR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 The Scenario\n",
        "Brazil is divided geopolitically into five macro-regions: North, North-East, Central-West, South-East, and South. For this case study, we will be acting as government officials in the Central-West Region of Brazil. Central-West Brazil is the smallest region in the country by population. It is also one of the larger and more rural regions. This means the number of COVID-19 patient records is significantly smaller compared to the larger regions.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1880Ux6qEb3MQSAirr4_908p20mvqQGot\" alt=\"Brazil Region Map\" width=\"500\"/>\n",
        "\n",
        "COVID-19 hit different regions at different time. Cases peaked later in the Central-West than in the more densely-populated and well-connected regions. Giving us the problem of scarce data in terms of COVID-19 patients in the region, but the potential lifeline of having larger datasets from the other regions, which we can learn from in order to augment our dataset. We cannot simply train our model on the data from all regions, because there is significant co-variate shift between the different regions and so we will achieve a better classifier by training on solely Central-West data, even if it is synthetic."
      ],
      "metadata": {
        "id": "wrQgPsCQtjKU"
      },
      "id": "wrQgPsCQtjKU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4.5 Load the data\n",
        "Lets set it up as a classification task with a death at time horizon column, as we did in a previous case study."
      ],
      "metadata": {
        "id": "kVlNWTXetnTG"
      },
      "id": "kVlNWTXetnTG"
    },
    {
      "cell_type": "code",
      "source": [
        "time_horizon = 14\n",
        "X = pd.read_csv(f\"../data/Brazil_COVID/covid_normalised_numericalised.csv\")\n",
        "\n",
        "X.loc[(X[\"Days_hospital_to_outcome\"] <= time_horizon) & (X[\"is_dead\"] == 1), f\"is_dead_at_time_horizon={time_horizon}\"] = 1\n",
        "X.loc[(X[\"Days_hospital_to_outcome\"] > time_horizon), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
        "X.loc[(X[\"is_dead\"] == 0), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
        "X[f\"is_dead_at_time_horizon={time_horizon}\"] = X[f\"is_dead_at_time_horizon={time_horizon}\"].astype(int)\n",
        "\n",
        "X.drop(columns=[\"is_dead\", \"Days_hospital_to_outcome\"], inplace=True) # drop survival columns as they are not needed for a classification problem"
      ],
      "metadata": {
        "id": "pjnJo89HtukL"
      },
      "id": "pjnJo89HtukL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define a region mapper, which maps the region encoding to the real values. These can be found in `synthetic-data-lab/data/Brazil_COVID/Brazil_COVID_data.md`."
      ],
      "metadata": {
        "id": "dGtWiuAsuXgT"
      },
      "id": "dGtWiuAsuXgT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mappings from region index to region\n",
        "region_mapper = {\n",
        "    0: \"Central-West\",\n",
        "    1: \"North\",\n",
        "    2: \"North-East\",\n",
        "    3: \"South\",\n",
        "    4: \"South-East\",\n",
        "}"
      ],
      "metadata": {
        "id": "BlJ_ZhAWuZ_x"
      },
      "id": "BlJ_ZhAWuZ_x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are acting as officials from Central-West Brazil, we need to split the data into data from our region and data from other regions. We then drop Region column to simulate not knowing what region the data is from. It is either in our region's dataset or in the dataset for other regions."
      ],
      "metadata": {
        "id": "xOJ1ZKWOuaea"
      },
      "id": "xOJ1ZKWOuaea"
    },
    {
      "cell_type": "code",
      "source": [
        "our_region_index = 0\n",
        "\n",
        "X_our_region_only = X.loc[X[\"Region\"] == our_region_index].copy()\n",
        "X_other_regions = X.loc[X[\"Region\"] != our_region_index].copy()\n",
        "X_all_regions = X.copy()\n",
        "\n",
        "display(X_our_region_only)"
      ],
      "metadata": {
        "id": "SFmJsvLfuc1-"
      },
      "id": "SFmJsvLfuc1-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 The problem\n",
        "\n",
        "Lets see how a model trained just on our data from the Central-West region performs."
      ],
      "metadata": {
        "id": "eT9SIHr7ufOz"
      },
      "id": "eT9SIHr7ufOz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6.1 Set up the data splits using train_test_split from sklearn."
      ],
      "metadata": {
        "id": "dIfcrslR51dT"
      },
      "id": "dIfcrslR51dT"
    },
    {
      "cell_type": "code",
      "source": [
        "our_region_y = X_our_region_only[\"is_dead_at_time_horizon=14\"]\n",
        "our_region_X = X_our_region_only.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(our_region_X, our_region_y, random_state=4)\n",
        "X_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "u6_H-ggizoWN"
      },
      "id": "u6_H-ggizoWN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6.2 Load classifier\n",
        "Load the trained xgboost classifier, which has been trained on Central-West data only."
      ],
      "metadata": {
        "id": "iG4GqJ0pzqxe"
      },
      "id": "iG4GqJ0pzqxe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=1,\n",
        "    gamma=1,\n",
        "    objective=\"binary:logistic\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Load the model trained on the whole dataset\n",
        "saved_model_path = AUG_RES_PATH / f\"augmentation_xgboost_real_{region_mapper[our_region_index]}_data.json\"\n",
        "xgb_model.load_model(saved_model_path)\n",
        "\n",
        "# # # The saved model was trained with the following code:\n",
        "# xgb_model.fit(X_train, y_train)\n",
        "# xgb_model.save_model(saved_model_path)"
      ],
      "metadata": {
        "id": "69WgAlPhzutC"
      },
      "id": "69WgAlPhzutC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6.3 Evaluate classifier\n",
        "Now print the performance of the model trained on only Central-West data."
      ],
      "metadata": {
        "id": "wCPqGbBuzw9l"
      },
      "id": "wCPqGbBuzw9l"
    },
    {
      "cell_type": "code",
      "source": [
        "calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"Evaluating accuracy: train set: {calculated_accuracy_score_train:0.4f} | test set: {calculated_accuracy_score_test:0.4f}\")"
      ],
      "metadata": {
        "id": "89-SsafGz0ER"
      },
      "id": "89-SsafGz0ER",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see we are significantly over-fitting due to the very small dataset. The performance does not look as good as it could be."
      ],
      "metadata": {
        "id": "4hOaWrw9z12q"
      },
      "id": "4hOaWrw9z12q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.7 A simple solution? Concatenation, not Augmentation\n",
        "\n",
        "One simple solution is to just concatenate the dataset with data from the other regions, but this will not account for any difference in the populations."
      ],
      "metadata": {
        "id": "sX4OMJ6-0Nb-"
      },
      "id": "sX4OMJ6-0Nb-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.7.1 Comparing the populations\n",
        "Let's examine how similar (or not) the populations are by plotting the distributions of a few of the data fields for each region."
      ],
      "metadata": {
        "id": "-_NTr3YHU6_w"
      },
      "id": "-_NTr3YHU6_w"
    },
    {
      "cell_type": "code",
      "source": [
        "# List columns by type to plot differently\n",
        "continuous_columns = [\"Age\"]\n",
        "descrete_cols = [\n",
        "    \"Sex\",\n",
        "    \"Fever\",\n",
        "    \"Cough\",\n",
        "    \"Sore_throat\",\n",
        "    \"Shortness_of_breath\",\n",
        "    \"Respiratory_discomfort\",\n",
        "    \"SPO2\",\n",
        "    \"Dihareea\",\n",
        "    \"Vomitting\",\n",
        "    \"Cardiovascular\",\n",
        "    \"Asthma\",\n",
        "    \"Diabetis\",\n",
        "    \"Pulmonary\",\n",
        "    \"Immunosuppresion\",\n",
        "    \"Obesity\",\n",
        "    \"Liver\",\n",
        "    \"Neurologic\",\n",
        "    \"Renal\",\n",
        "]\n",
        "\n",
        "# Plot continuous columns\n",
        "for column in continuous_columns:\n",
        "    X_all_regions.groupby(\"Region\")[column].plot(kind=\"kde\")\n",
        "    plt.legend(region_mapper.values(), title=\"Region\")\n",
        "    plt.title(f\"{column}\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot Ethnicity separately as it is our focus in this fairness exercise\n",
        "column = \"Ethnicity\"\n",
        "dft = X_all_regions.replace({\"Region\": region_mapper}).melt(id_vars=[\"Region\"]).loc[\n",
        "    X_all_regions.melt()[\"variable\"] == column\n",
        "]\n",
        "dfa = dft.value_counts().reset_index()\n",
        "# Rename columns for clarity, include the original column name\n",
        "dfa.columns = ['Region', 'variable', 'value', 'count_a']\n",
        "\n",
        "dfb = dft.value_counts(subset=[\"Region\"]).reset_index()\n",
        "dfb.columns = ['Region', 'count_b']  # Rename columns for clarity\n",
        "\n",
        "dfc = dfa.merge(dfb, on='Region', how='inner')\n",
        "dfc['prob'] = dfc['count_a'] / dfc['count_b']\n",
        "\n",
        "sns.barplot(\n",
        "    data=dfc,\n",
        "    x=\"value\",\n",
        "    y=\"prob\",\n",
        "    hue=\"Region\",\n",
        ")\n",
        "plt.title(f\"{column}\")\n",
        "plt.show()\n",
        "\n",
        "# Plot all other discrete columns in grid\n",
        "fig, ax = plt.subplots(nrows=3, ncols=6, figsize=(35, 20))\n",
        "for idx, column in enumerate(descrete_cols):\n",
        "    dft = X_all_regions.replace({\"Region\": region_mapper}).melt(id_vars=[\"Region\"]).loc[\n",
        "        X_all_regions.melt()[\"variable\"] == column\n",
        "    ]\n",
        "    dfa = dft.value_counts().reset_index()\n",
        "    dfa.columns = ['Region', 'variable', 'value', 'count_a']  # Rename columns for clarity\n",
        "\n",
        "    dfb = dft.value_counts(subset=[\"Region\"]).reset_index()\n",
        "    dfb.columns = ['Region', 'count_b']  # Rename columns for clarity\n",
        "\n",
        "    dfc = dfa.merge(dfb, on='Region', how='inner')\n",
        "    dfc['prob'] = dfc['count_a'] / dfc['count_b']\n",
        "\n",
        "    sns.barplot(\n",
        "        data=dfc,\n",
        "        x='value',\n",
        "        y='prob',\n",
        "        ax=ax[idx // 6][idx % 6],\n",
        "        hue=\"Region\",\n",
        "    )\n",
        "    ax[idx // 6][idx % 6].title.set_text(f\"{column}\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_HGkAcM9U4DN"
      },
      "id": "_HGkAcM9U4DN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing the plots above it appears that there are some differences in the populations. The distributions for the symptoms and co-morbidities seem to have different shapes in many cases. To pick a few examples, having a renal co-morbidity is much more common in the North or North-East than South or Central-West regions; the symptom of cough is more common in the North than other regions; and the Central-West region seems to be on average younger than other regions. There are other examples we could have listed as well. Spend a moment reviewing these plots to spot differences for youself."
      ],
      "metadata": {
        "id": "BYYTg9-Okt-V"
      },
      "id": "BYYTg9-Okt-V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.7.2 Set up the training and testing data sets for the model\n",
        "\n",
        "Make sure the training sets come from the all region dataset, but the test sets come from our region."
      ],
      "metadata": {
        "id": "xonVEMiV0PS8"
      },
      "id": "xonVEMiV0PS8"
    },
    {
      "cell_type": "code",
      "source": [
        "# drop Region column to simulate a simple concatenation of two datasets, one from the Central-West, one from the rest of Brazil.\n",
        "X_our_region_only_for_baseline = X_our_region_only.drop(columns=[\"Region\"])\n",
        "X_all_regions_for_baseline = X_all_regions.drop(columns=[\"Region\"])\n",
        "\n",
        "concat_y = X_all_regions_for_baseline[\"is_dead_at_time_horizon=14\"]\n",
        "concat_X = X_all_regions_for_baseline.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
        "\n",
        "X_train, _, y_train, _ = train_test_split(concat_X, concat_y, random_state=4)\n",
        "X_train.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "our_region_y = X_our_region_only_for_baseline[\"is_dead_at_time_horizon=14\"]\n",
        "our_region_X = X_our_region_only_for_baseline.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
        "\n",
        "_, X_test, _, y_test = train_test_split(our_region_X, our_region_y, random_state=4)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "5UaSZZ-f0g1l"
      },
      "id": "5UaSZZ-f0g1l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.7.3 Load the model trained on data from all regions."
      ],
      "metadata": {
        "id": "duLJH-Gr0ibq"
      },
      "id": "duLJH-Gr0ibq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=1,\n",
        "    gamma=1,\n",
        "    objective=\"binary:logistic\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Load the model trained on the whole dataset\n",
        "# saved_model_path = AUG_RES_PATH / f\"augmentation_xgboost_real_all_data.json\"\n",
        "# xgb_model.load_model(saved_model_path)\n",
        "\n",
        "# # The saved model was trained with the following code:\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_model.save_model(saved_model_path)"
      ],
      "metadata": {
        "id": "GIYCGQ2V0mqB"
      },
      "id": "GIYCGQ2V0mqB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.7.4 Show the performance of the model."
      ],
      "metadata": {
        "id": "3Hjl1R9u0org"
      },
      "id": "3Hjl1R9u0org"
    },
    {
      "cell_type": "code",
      "source": [
        "calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"Evaluating accuracy: train set: {calculated_accuracy_score_train:0.4f} | test set: {calculated_accuracy_score_test:0.4f}\")"
      ],
      "metadata": {
        "id": "0NL3rqmQ0sL8"
      },
      "id": "0NL3rqmQ0sL8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see our accuracy does improve, but we can do better!\n",
        "\n",
        "Why else my this not be the best, or even a possible approach?\n",
        "<details>\n",
        "<summary>Show answer</summary>\n",
        "There may well be cases where there is a greater co-variate shift that impacts this accuracy to a much greater extent, i.e. the population of one region may be much more different compared to the overall population in some settings. It is also worth bearing in mind that there are contexts where the above approach is not even an option, such as in the case of only partially overlapping (or missing) features. Use cannot concatenate datasets that don't share columns, but using a RadialGAN to augment a dataset like this is still possible.\n",
        "</details>"
      ],
      "metadata": {
        "id": "V4fof2rK0uP8"
      },
      "id": "V4fof2rK0uP8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.8 The Solution\n",
        "\n",
        "Augment this dataset with the use of a RadialGAN."
      ],
      "metadata": {
        "id": "AbVhhnLv0xYM"
      },
      "id": "AbVhhnLv0xYM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.8.1 Load the data\n",
        "\n",
        "First, lets load the super-set of data from all regions into the `GenericDataLoader` object."
      ],
      "metadata": {
        "id": "ZFv8NJ8U0zqp"
      },
      "id": "ZFv8NJ8U0zqp"
    },
    {
      "cell_type": "code",
      "source": [
        "loader = GenericDataLoader(\n",
        "    X, # X is the dataframe which is a superset of all region data\n",
        "    target_column=\"is_dead_at_time_horizon=14\", # The column containing the labels to predict\n",
        "    sensitive_features=[\"Ethnicity\"], # The sensitive features in the dataset\n",
        "    domain_column=\"Region\", # This labels the domain that each record is from. Where it is `0` it is from our small dataset.\n",
        "    random_state=42,\n",
        ")"
      ],
      "metadata": {
        "id": "TjI1kWUM07vI"
      },
      "id": "TjI1kWUM07vI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.8.2 Load/Create the synthetic data model\n",
        "Lets use a RadialGan to augment the data. We need to load the plugin and then fit it to the dataloader object."
      ],
      "metadata": {
        "id": "hYQ2qk2u09rh"
      },
      "id": "hYQ2qk2u09rh"
    },
    {
      "cell_type": "code",
      "source": [
        "outdir = Path(\"saved_models\")\n",
        "prefix = \"augmentation\"\n",
        "model=\"radialgan\"\n",
        "n_iter = 49\n",
        "random_state = 8"
      ],
      "metadata": {
        "id": "MqdfqQXa1B8p"
      },
      "id": "MqdfqQXa1B8p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now either create and fit a synthetic data model then save that model to file, or load one we have already saved from file."
      ],
      "metadata": {
        "id": "WIxvbgUf1GAD"
      },
      "id": "WIxvbgUf1GAD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define saved model name\n",
        "save_file = outdir / f\"{prefix}.{model}_numericalised_{region_mapper[our_region_index]}_n_iter={n_iter}_rnd={random_state}_final.bkp\"\n",
        "# Load if available\n",
        "if Path(save_file).exists():\n",
        "    syn_model = serialization.load_from_file(save_file)\n",
        "# Create if not available\n",
        "else:\n",
        "  syn_model = Plugins().get(model, n_iter=n_iter, random_state=random_state)\n",
        "  syn_model.fit(loader)\n",
        "  serialization.save_to_file(save_file, syn_model)"
      ],
      "metadata": {
        "id": "cgPJQAHg1GU7"
      },
      "id": "cgPJQAHg1GU7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.8.3 Augment the dataset\n",
        "\n",
        "Lets use our synthetic model to generate some data and use it to augment our original dataset."
      ],
      "metadata": {
        "id": "U6GOSsmk1X0J"
      },
      "id": "U6GOSsmk1X0J"
    },
    {
      "cell_type": "code",
      "source": [
        "n_gen_records = 500\n",
        "\n",
        "synth_data = syn_model.generate(n_gen_records, domains=[our_region_index], random_state=random_state)\n",
        "\n",
        "# Now we can augment our original dataset with our new synthetic data\n",
        "augmented_data = pd.concat([\n",
        "    synth_data.dataframe(),\n",
        "    X_our_region_only,\n",
        "])\n",
        "\n",
        "print(f\"{len(synth_data['Region'])} synthetic records generated.\")\n",
        "print(f\"{len(X_our_region_only['Region'])} original real records.\")\n",
        "print(f\"{len(X_our_region_only['Region']) + len(synth_data['Region'])} records in the augmented dataset.\")"
      ],
      "metadata": {
        "id": "Zv59KyQw1dML"
      },
      "id": "Zv59KyQw1dML",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.8.4 Set up the data for the classifier\n",
        "Now we need to test a model trained on the augmented dataset, so we need to set up our data splits again."
      ],
      "metadata": {
        "id": "2kb221Eb1fDq"
      },
      "id": "2kb221Eb1fDq"
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_y = augmented_data[\"is_dead_at_time_horizon=14\"]\n",
        "augmented_X = augmented_data.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
        "\n",
        "augmented_y.reset_index(drop=True, inplace=True)\n",
        "augmented_X.reset_index(drop=True, inplace=True)\n",
        "\n",
        "our_region_y = X_our_region_only[\"is_dead_at_time_horizon=14\"]\n",
        "our_region_X = X_our_region_only.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
        "\n",
        "_, X_test, _, y_test = train_test_split(our_region_X, our_region_y, random_state=4)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "eBS-3FnC1i-Z"
      },
      "id": "eBS-3FnC1i-Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.8.5 Load the trained model"
      ],
      "metadata": {
        "id": "1WRjhCNP1k6a"
      },
      "id": "1WRjhCNP1k6a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=1,\n",
        "    gamma=1,\n",
        "    objective=\"binary:logistic\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# # Load the model trained on the whole dataset\n",
        "# saved_model_path = AUG_RES_PATH / f\"augmentation_xgboost_augmented_data.json\"\n",
        "# xgb_model.load_model(saved_model_path)\n",
        "\n",
        "# The saved model was trained with the following code:\n",
        "xgb_model.fit(augmented_X, augmented_y)\n",
        "xgb_model.save_model(saved_model_path)"
      ],
      "metadata": {
        "id": "kvBLhUmB1pLJ"
      },
      "id": "kvBLhUmB1pLJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = xgb_model.predict(augmented_X)\n",
        "calculated_accuracy_score_test = accuracy_score(augmented_y, y_pred)\n",
        "print(f\"Accuracy on training set (augmented data): {calculated_accuracy_score_test:0.4f}\")\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on real data test set: {calculated_accuracy_score_test:0.4f}\")"
      ],
      "metadata": {
        "id": "MWaIQhR31uH-"
      },
      "id": "MWaIQhR31uH-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.8.7 Results\n",
        "\n",
        "The model over-fitting on the training data is significantly reduced and the accuracy that is much higher than for the small dataset comprised solely of data from the Central-West region. We also see a significant improvement over training the model on the superset of the real data."
      ],
      "metadata": {
        "id": "ZsU-3JZe1rFx"
      },
      "id": "ZsU-3JZe1rFx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.9 Extension\n",
        "Use the code block below as a space to complete the extension exercises below."
      ],
      "metadata": {
        "id": "KUrVVJuR1ylO"
      },
      "id": "KUrVVJuR1ylO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.9.1 Can you generate some more augmented datasets to answer the following questions?\n",
        "1) How much synthetic data should you create for best results? Is there a minimum value that you would use? Is there a maximum value? In each case, why does that minium/maximum occur?\n",
        "<br/>2) How much does changing the RadialGan plugin parameter `n_iter` change the quality of the generated data?"
      ],
      "metadata": {
        "id": "mEmB9lt910qq"
      },
      "id": "mEmB9lt910qq"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ravHlMRR2HC1"
      },
      "id": "ravHlMRR2HC1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (i) How many synthetic records should you create?\n",
        "accuracies = []\n",
        "generated_records = [\n",
        "    100,\n",
        "    300,\n",
        "    500,\n",
        "    700,\n",
        "    900,\n",
        "    1000,\n",
        "    2000,\n",
        "    3000,\n",
        "    4000,\n",
        "    5000,\n",
        "    8000,\n",
        "    10000,\n",
        "    # 15000,\n",
        "    # 20000,\n",
        "    # 50000,\n",
        "    # 100000,\n",
        "    # 1000000,\n",
        "] # Larger values take longer to run\n",
        "repeats = 1 # Can be set higher to reduce the variance by using mean value\n",
        "for n_gen_records in generated_records:\n",
        "    rep_vals = []\n",
        "    for i in range(repeats):\n",
        "        synth_data = syn_model.generate(n_gen_records, domains=[our_region_index])\n",
        "\n",
        "        # Now we can augment our original dataset with our new synthetic data\n",
        "        augmented_data = pd.concat([\n",
        "            synth_data.dataframe(),\n",
        "            X_our_region_only,\n",
        "        ])\n",
        "\n",
        "        augmented_y = augmented_data[\"is_dead_at_time_horizon=14\"]\n",
        "        augmented_X_in = augmented_data.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(augmented_X_in, augmented_y, random_state=4)\n",
        "        X_train.reset_index(drop=True, inplace=True)\n",
        "        X_test.reset_index(drop=True, inplace=True)\n",
        "        y_train.reset_index(drop=True, inplace=True)\n",
        "        y_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # Train model on whole dataset\n",
        "        xgb_model = xgb.XGBClassifier(\n",
        "            n_estimators=2000,\n",
        "            learning_rate=0.01,\n",
        "            max_depth=5,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=1,\n",
        "            gamma=1,\n",
        "            objective=\"binary:logistic\",\n",
        "            random_state=42,\n",
        "        )\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = xgb_model.predict(X_test)\n",
        "        calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
        "        calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
        "        # print(f\"Evaluating accuracy: n_gen_records: {n_gen_records} train set: {calculated_accuracy_score_train}| test set: {calculated_accuracy_score_test}\")\n",
        "        rep_vals.append(calculated_accuracy_score_test)\n",
        "    accuracies.append(np.mean(rep_vals))\n",
        "\n",
        "d = {\"generated_records\": generated_records, \"accuracies\": accuracies}\n",
        "accuracy_data= pd.DataFrame(d)\n",
        "plot = sns.lineplot(\n",
        "    y=\"accuracies\",\n",
        "    x=\"generated_records\",\n",
        "    data=accuracy_data\n",
        ").set(title=f\"Augmenting {region_mapper[our_region_index]}, n_iter={500}, without {'original data'}\")\n"
      ],
      "metadata": {
        "id": "Axp3OnvG2Moq",
        "cellView": "form"
      },
      "id": "Axp3OnvG2Moq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.10 Benchmarking augmentation\n",
        "Use the benchmarking interface (documented [here](https://synthcity.readthedocs.io/en/latest/generated/synthcity.benchmark.html) and covered in [tutorial 2](https://colab.research.google.com/github/vanderschaarlab/synthcity/blob/main/tutorials/tutorial2_benchmarks.ipynb)) to see if you can improve the performance of a downstream classifer. You may have to use different models and parameters. There is some code to start you off below, but you are encouraged to change it up and experiment."
      ],
      "metadata": {
        "id": "v7J15FocfU7l"
      },
      "id": "v7J15FocfU7l"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helpful code to start you off\n",
        "# stdlib\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# third party\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# synthcity absolute\n",
        "import synthcity.logger as log\n",
        "from synthcity.plugins import Plugins\n",
        "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
        "from synthcity.benchmark import Benchmarks\n",
        "\n",
        "\"\"\" You can limit this to just the augmentation metrics by passing a dictionary\n",
        "of the the desired metrics, as we did in section 3.6 of this notebook\"\"\"\n",
        "\n",
        "score = Benchmarks.evaluate(\n",
        "    [(\"ctgan\", \"ctgan\", {})],\n",
        "    loader,\n",
        "    synthetic_size=len(X),\n",
        "    augmentation_rule=\"equal\",\n",
        "    strict_augmentation=True,\n",
        "    repeats=1,\n",
        ")\n",
        "Benchmarks.print(score)"
      ],
      "metadata": {
        "id": "tJzWNjuVfS3H"
      },
      "id": "tJzWNjuVfS3H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ji-LdyAbVCB0"
      },
      "id": "Ji-LdyAbVCB0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "8b1180d7559eadeaa51f0c23b115f584a6e0cc67e9bc1d662a0e6b39392000a4"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}